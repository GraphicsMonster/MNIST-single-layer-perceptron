{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34877,"sourceType":"datasetVersion","datasetId":27352}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-05T14:16:42.268376Z","iopub.execute_input":"2024-01-05T14:16:42.268849Z","iopub.status.idle":"2024-01-05T14:16:42.275234Z","shell.execute_reply.started":"2024-01-05T14:16:42.268814Z","shell.execute_reply":"2024-01-05T14:16:42.273317Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"train_set = np.array(pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv'))[:1000]\ntest_set = np.array(pd.read_csv('/kaggle/input/mnist-in-csv/mnist_test.csv'))[2000:3000]","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:16:42.318873Z","iopub.execute_input":"2024-01-05T14:16:42.319854Z","iopub.status.idle":"2024-01-05T14:16:47.571221Z","shell.execute_reply.started":"2024-01-05T14:16:42.319811Z","shell.execute_reply":"2024-01-05T14:16:47.569993Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"train_labels = train_set[:, 0].T\ntest_labels = test_set[:, 0].T\nX_train = train_set[:, 1:].T\nX_test = test_set[:, 1:].T\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_label shape: \", train_labels.shape)\nprint(\"training labels shape: \", train_labels.shape)\nprint(\"test labels shape: \", test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:16:47.573800Z","iopub.execute_input":"2024-01-05T14:16:47.574285Z","iopub.status.idle":"2024-01-05T14:16:47.583605Z","shell.execute_reply.started":"2024-01-05T14:16:47.574242Z","shell.execute_reply":"2024-01-05T14:16:47.582002Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"X_train shape:  (784, 1000)\nX_label shape:  (1000,)\ntraining labels shape:  (1000,)\ntest labels shape:  (1000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# initialize parameters\ndef init_params():\n    w1 = np.random.rand(784, 10)\n    b1 = np.random.rand(10, 1)\n    w2 = np.random.rand(10, 10)\n    b2 = np.random.rand(10, 1)\n    return w1, b1, w2, b2","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:16:47.585056Z","iopub.execute_input":"2024-01-05T14:16:47.585549Z","iopub.status.idle":"2024-01-05T14:16:47.599895Z","shell.execute_reply.started":"2024-01-05T14:16:47.585511Z","shell.execute_reply":"2024-01-05T14:16:47.598674Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# Defining activation functions here\ndef ReLU(x):\n    return np.maximum(0, x)\n\ndef Softmax(x):\n    exp_x = np.exp(x - np.max(x))\n    return exp_x/exp_x.sum()","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:16:47.602289Z","iopub.execute_input":"2024-01-05T14:16:47.603258Z","iopub.status.idle":"2024-01-05T14:16:47.611211Z","shell.execute_reply.started":"2024-01-05T14:16:47.603209Z","shell.execute_reply":"2024-01-05T14:16:47.609921Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# defining the derivatives of the activations used\ndef der_ReLU(x):\n    return np.where(x <= 0, 0, 1)\n    \ndef der_Softmax(z):\n    exps = np.exp(z)\n    sum_exps = np.sum(exps, axis=1, keepdims=True)  # Sum along the class dimension\n    softmax = exps / sum_exps\n\n    return softmax * (1 - softmax)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:16:47.612947Z","iopub.execute_input":"2024-01-05T14:16:47.613602Z","iopub.status.idle":"2024-01-05T14:16:47.620705Z","shell.execute_reply.started":"2024-01-05T14:16:47.613568Z","shell.execute_reply":"2024-01-05T14:16:47.619057Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"def one_hot_encode(Y):\n    return np.eye(10)[Y].reshape(-1, 1)\n\nprint(one_hot_encode(5))","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:16:47.622572Z","iopub.execute_input":"2024-01-05T14:16:47.623708Z","iopub.status.idle":"2024-01-05T14:16:47.632601Z","shell.execute_reply.started":"2024-01-05T14:16:47.623668Z","shell.execute_reply":"2024-01-05T14:16:47.631447Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"[[0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Defining the loss function\n# The labels have to be one hot encoded\ndef cost(output, label):\n    epsilon = 1e-10  # small constant to avoid log(0)\n    return -np.sum(label * np.log(output + epsilon)) / label.shape[1]\n\ndef cost_derivative(output, label):\n    return 2*(output-label)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:16:47.635141Z","iopub.execute_input":"2024-01-05T14:16:47.635583Z","iopub.status.idle":"2024-01-05T14:16:47.643952Z","shell.execute_reply.started":"2024-01-05T14:16:47.635549Z","shell.execute_reply":"2024-01-05T14:16:47.642714Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# Forward pass\ndef forward(x, w1, b1, w2, b2):\n    Z1 = w1.T.dot(x) + b1\n    A1 = ReLU(Z1)\n    Z2 = w2.dot(A1) + b2\n    A2 = Softmax(Z2)\n    return Z1, A1, Z2, A2","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:16:47.645593Z","iopub.execute_input":"2024-01-05T14:16:47.647089Z","iopub.status.idle":"2024-01-05T14:16:47.654313Z","shell.execute_reply.started":"2024-01-05T14:16:47.646929Z","shell.execute_reply":"2024-01-05T14:16:47.652773Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"# Backward pass\ndef backward(X, Z2, A2, A1, Z1, W2, label):\n    dz2 = Z2 - label\n    dw2 = dz2.dot(Z1.T)\n    db2 = np.sum(dz2, axis=1, keepdims=True)\n    \n    #the derivative wrt z1\n    dz1 = W2.T.dot(dz2) * der_ReLU(A1)\n    \n    dw1 = dz1.dot(X.T)\n    db1 = np.sum(dz1, axis=1, keepdims=True)\n    return dw1, db1, dw2, db2","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:16:47.656246Z","iopub.execute_input":"2024-01-05T14:16:47.657803Z","iopub.status.idle":"2024-01-05T14:16:47.666418Z","shell.execute_reply.started":"2024-01-05T14:16:47.657757Z","shell.execute_reply":"2024-01-05T14:16:47.665313Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# Update parameters\ndef update_params(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha):\n    w1 = w1 - (alpha * dw1.T)\n    w2 = w2 - (alpha * dw2)\n    b1 = b1 - (alpha * db1)\n    b2 = b2 - (alpha * db2)\n    \n    return w1, b1, w2, b2","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:16:47.670445Z","iopub.execute_input":"2024-01-05T14:16:47.671477Z","iopub.status.idle":"2024-01-05T14:16:47.678658Z","shell.execute_reply.started":"2024-01-05T14:16:47.671400Z","shell.execute_reply":"2024-01-05T14:16:47.677475Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"def gradient_descent(X, Y, num_epochs, alpha):\n    \"\"\"Performs stochastic gradient descent for a specified number of epochs.\"\"\"\n\n    w1, b1, w2, b2 = init_params()\n    loss = 0.0\n    for epoch in range(num_epochs):\n        for i in range(X.shape[1]):  # Iterate over samples\n            sample_X = X[:, i].reshape(-1, 1)  # Reshape to column vector\n            sample_Y = Y[i]\n            sample_Y = one_hot_encode(sample_Y)\n\n            # Perform forward pass, backward pass, and updates for this sample\n            Z1, A1, Z2, A2 = forward(sample_X, w1, b1, w2, b2)\n            dw1, db1, dw2, db2 = backward(sample_X, Z2, A2, A1, Z1, w2, sample_Y)\n            w1, b1, w2, b2 = update_params(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha)\n            loss = cost(Z2, sample_Y)\n\n        # Print loss periodically\n        if epoch % 50== 0:  # Print loss every 50 samples\n            print(\"Epoch:\", epoch, \"Loss:\", loss)\n\n    return w1, b1, w2, b2\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:16:47.680729Z","iopub.execute_input":"2024-01-05T14:16:47.681631Z","iopub.status.idle":"2024-01-05T14:16:47.694315Z","shell.execute_reply.started":"2024-01-05T14:16:47.681583Z","shell.execute_reply":"2024-01-05T14:16:47.692977Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"w1, b1, w2, b2 = gradient_descent(X_train, train_labels, 500, 0.01)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:16:47.696436Z","iopub.execute_input":"2024-01-05T14:16:47.696927Z","iopub.status.idle":"2024-01-05T14:17:45.758643Z","shell.execute_reply.started":"2024-01-05T14:16:47.696884Z","shell.execute_reply":"2024-01-05T14:17:45.756740Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"Epoch:","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_42/2436537732.py:5: RuntimeWarning: invalid value encountered in log\n  return -np.sum(label * np.log(output + epsilon)) / label.shape[1]\n","output_type":"stream"},{"name":"stdout","text":" 0 Loss: 2.868382809752485\nEpoch: 50 Loss: 2.2615720040274017\nEpoch: 100 Loss: 2.2615720040274017\nEpoch: 150 Loss: 2.2615720040274017\nEpoch: 200 Loss: 2.2615720040274017\nEpoch: 250 Loss: 2.2615720040274017\nEpoch: 300 Loss: 2.2615720040274017\nEpoch: 350 Loss: 2.2615720040274017\nEpoch: 400 Loss: 2.2615720040274017\nEpoch: 450 Loss: 2.2615720040274017\n","output_type":"stream"}]}]}